{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "599ba01a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#sys.path.append(os.path.dirname(SCRIPT_DIR))\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tbilby\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import bilby \n",
    "from gwpy.timeseries import TimeSeries\n",
    "from scipy import stats\n",
    "from bilby.core.prior import  Uniform, Interped\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bilby.core.prior import  LogUniform\n",
    "from scipy import interpolate\n",
    "import inspect\n",
    "from scipy.signal import find_peaks_cwt\n",
    "from scipy import signal\n",
    "import gwpy\n",
    "from bilby.core.utils import infer_parameters_from_function\n",
    "from context import tbilby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3863777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some settings \n",
    "label='tBiliby_ASD_GW150914'\n",
    "\n",
    "trigger_time = 1126259462.4\n",
    "maximum_frequency = 896\n",
    "minimum_frequency = 20\n",
    "roll_off = 0.4  \n",
    "duration = 4  # Analysis segment duration\n",
    "post_trigger_duration = 2  # Time between trigger time and end of segment\n",
    "end_time = trigger_time + post_trigger_duration\n",
    "start_time = end_time - duration\n",
    "det='H1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd65479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define some functions to help out  \n",
    "def get_GW_data(f_i,f_f,median=False):\n",
    "    \n",
    "    global minimum_frequency\n",
    "    global maximum_frequency\n",
    "    global duration\n",
    "    global det \n",
    "    \n",
    "    if minimum_frequency < f_i:\n",
    "        minimum_frequency = f_i \n",
    "    if maximum_frequency > f_f:\n",
    "        maximum_frequency = f_f\n",
    "    \n",
    "    \n",
    "    if median:\n",
    "        psd_duration = 32 * duration\n",
    "    else:\n",
    "        psd_duration = duration \n",
    "        \n",
    "    psd_start_time = start_time - psd_duration\n",
    "    psd_end_time = start_time\n",
    "       \n",
    "    psd_data = gwpy.timeseries.TimeSeries.fetch_open_data(det, psd_start_time, psd_end_time)    \n",
    "    psd_alpha = 2 * roll_off / duration\n",
    "    psd = psd_data.psd(\n",
    "        fftlength=duration, overlap=0, window=(\"tukey\", psd_alpha), method=\"median\"\n",
    "    )\n",
    "  \n",
    "    psd_frequencies = psd.frequencies.value\n",
    "    psd = psd.value\n",
    "    I = (psd_frequencies > minimum_frequency) &  (psd_frequencies < maximum_frequency)         \n",
    "    return psd_frequencies[I],np.sqrt(psd[I])\n",
    "\n",
    "\n",
    "def max_rolling(a, window,axis =1):\n",
    "        shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "        strides = a.strides + (a.strides[-1],)\n",
    "        rolling = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "        return np.max(rolling,axis=axis)\n",
    "    \n",
    "def min_rolling(a, window,axis =1):\n",
    "        shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "        strides = a.strides + (a.strides[-1],)\n",
    "        rolling = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "        return np.min(rolling,axis=axis)\n",
    "\n",
    "def med_rolling(a, window,axis =1):\n",
    "        shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "        strides = a.strides + (a.strides[-1],)\n",
    "        rolling = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "        return np.median(rolling,axis=axis)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad97be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some tbilby conditional priors\n",
    "\n",
    "class TransdimensionalConditionalBeta_Amp(tbilby.core.prior.TransdimensionalConditionalBeta):\n",
    " \n",
    "    def set_extra_data(self,x,y):\n",
    "        self.y=y\n",
    "        self.x=x \n",
    "        # build interpolation map x-> min, x-> max \n",
    "        y_max = max_rolling(y,20)\n",
    "        y_min = min_rolling(y,20)\n",
    "        y_med = med_rolling(y,101)\n",
    "        self.fdata = interpolate.interp1d(x, self.y*1.05,fill_value=\"extrapolate\")\n",
    "        self.fmin = interpolate.interp1d(x[10:-9], y_min,fill_value=\"extrapolate\")\n",
    "        self.fmax = interpolate.interp1d(x[50:-50], y[50:-50] - y_med,fill_value=\"extrapolate\")\n",
    "            \n",
    "        self.Nsigma = 2 # for now let be conservative \n",
    "    \n",
    "       \n",
    "    # this uses the full power of the conditional priror, it is based on the smooth function evaluation and teh location of the line       \n",
    "    # we check if the data's amplitude is large enuogh to account for a line.   \n",
    "    def transdimensional_condition_function(self,**required_variables):        \n",
    "            componant_function_number= len(self.Amp)            \n",
    "            #extract its location \n",
    "            x_val = self.loc[componant_function_number]\n",
    "        \n",
    "            if isinstance(x_val, float):\n",
    "                x_val=[x_val]\n",
    "                \n",
    "            minimum = self.fmin(x_val) \n",
    "            maximum = self.fmax(x_val)\n",
    "            maximum_data = self.fdata(x_val)\n",
    "            \n",
    "            est_smooth_func=np.zeros(np.array(x_val).shape)\n",
    "            for n in np.arange(np.max(np.max(self.n_exp)).astype(int)):\n",
    "                est_smooth_func += (self.n_exp > n).astype(float)*exp(x_val,self.A[n],self.lamda[n])   \n",
    "            \n",
    "            \n",
    "            new_minimum = est_smooth_func * self.Nsigma           \n",
    "            minimum = est_smooth_func * (self.Nsigma-1) # at least the number of sigmas from the smooth fit                          \n",
    "            maximum  = maximum_data - est_smooth_func\n",
    "            \n",
    "            if isinstance(new_minimum,np.ndarray):\n",
    "                too_big_in_japan =  new_minimum > maximum_data\n",
    "                minimum[too_big_in_japan] = 0.9\n",
    "                maximum[too_big_in_japan] = 1\n",
    "            else: # float \n",
    "                \n",
    "                if  new_minimum > maximum_data:\n",
    "                    maximum = np.array([1])\n",
    "                    minimum = np.array([0.9])               \n",
    "            if len(minimum)==1:\n",
    "                minimum=minimum[0]\n",
    "                maximum=maximum[0]\n",
    "\n",
    "                \n",
    "            return dict(minimum=minimum,maximum=maximum)\n",
    "        \n",
    "# actually, here this is not necessary, since we are not doing anything special.. a normal interpolation would work just as well          \n",
    "class TransdimensionalConditionalTransInterped_loc(tbilby.core.prior.TransdimensionalConditionalTransInterped):\n",
    "    def transdimensional_condition_function(self,**required_variables):\n",
    "        # setting the mimmum according the the last peak value\n",
    "            trans_min = self.minimum\n",
    "            trans_max = self.maximum\n",
    "            return dict(trans_min=trans_min,trans_max=trans_max) \n",
    "            \n",
    "    \n",
    "class TransdimensionalConditionalUniform_lamda(tbilby.core.prior.TransdimensionalConditionalUniform):   \n",
    "    def transdimensional_condition_function(self,**required_variables):\n",
    "        # setting the mimmum according the the last peak value\n",
    "            minimum = self.minimum\n",
    "            if(len(self.lamda)>0): # handle the first mu case\n",
    "                minimum = self.lamda[-1] # set the minimum to be the location of the last peak \n",
    "                           \n",
    "            return dict(minimum=minimum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43012cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model functions\n",
    "\n",
    "def exp(x,A,lamda):  \n",
    "    xfunc = x.copy() \n",
    "    return A*np.power(xfunc,lamda)\n",
    "\n",
    "\n",
    "def lorentzian(x, loc, Amp, gam):\n",
    "    # damped tails lorentzian, following bayeslines\n",
    "    decay =1\n",
    "    damped_alpha = np.zeros(x.shape) \n",
    "    df_i = loc/50\n",
    "    I_left_tail = x < loc - df_i\n",
    "    I_right_tail =x > loc + df_i   \n",
    "    damped_alpha [I_right_tail | I_left_tail] = decay \n",
    "    return np.exp(-damped_alpha*(x-df_i)/(df_i)) *Amp * gam**2 / ( gam**2 + ( x - loc )**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the likelihood function \n",
    "class ASD_GaussianLikelihood(bilby.Likelihood):\n",
    "    def __init__(self, x, y, function):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.N = len(x)\n",
    "\n",
    "        self.function = function\n",
    "\n",
    "        # These lines of code infer the parameters from the provided function\n",
    "        parameters = inspect.getfullargspec(function).args\n",
    "\n",
    "        del parameters[0]\n",
    "        super().__init__(parameters=dict.fromkeys(parameters))\n",
    "        self.parameters = dict.fromkeys(parameters)\n",
    "\n",
    "        self.function_keys = self.parameters.keys()    \n",
    "        if 'sigma' in self.function_keys: # remove sigma from function keys otherwise it get sent to the model\n",
    "           self.function_keys.pop['sigma']\n",
    "           print('Removing sigma')\n",
    "    \n",
    "    def log_likelihood(self):\n",
    "        model_parameters = {k: self.parameters[k] for k in self.function_keys if k != 'sigma'}        \n",
    "        est_asd = self.function(self.x, **model_parameters)             \n",
    "        log_l = -len(est_asd)*np.log(2*np.pi)/2 + np.sum(-0.5*(self.y/est_asd)**2 -np.log(est_asd))       \n",
    "        return log_l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef45a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing to reduce priro volume and save some time \n",
    "# some helper function \n",
    "def find_indices(larger_array, smaller_array):\n",
    "    indices = []\n",
    "    for item in smaller_array:\n",
    "        try:\n",
    "            index = np.where(larger_array == item)[0][0]\n",
    "            indices.append(index)\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return indices\n",
    "# load the data \n",
    "x,y= get_GW_data(f_i=450,f_f=550)\n",
    "# find the line with excess power of the smooth function which is estimated by the median, create a kde to estimate the lines distribution   \n",
    "sigmas=y[50:-50]/med_rolling(y,101)\n",
    "I = sigmas > 4\n",
    "x_mod = x[50:-50][I]\n",
    "peaks_new = find_indices(x,x_mod)\n",
    "\n",
    "I = sigmas > 3\n",
    "x_mod = x[50:-50][I]\n",
    "peaks_new= np.append(peaks_new,np.array(find_indices(x,x_mod)))\n",
    "\n",
    "I = sigmas > 2.5\n",
    "x_mod = x[50:-50][I]\n",
    "peaks_new= np.append(peaks_new,np.array(find_indices(x,x_mod)))\n",
    "\n",
    "kernel_new  = stats.gaussian_kde(x[peaks_new], bw_method=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_exp=5\n",
    "n_peaks=12\n",
    "\n",
    "componant_functions_dict={}\n",
    "componant_functions_dict[exp]=(n_exp,'A','lamda')\n",
    "componant_functions_dict[lorentzian]=(n_peaks,'loc', 'Amp', 'gam')\n",
    "\n",
    "model = tbilby.create_transdimensional_model('model',  componant_functions_dict,returns_polarization=False,SaveTofile=False)\n",
    "\n",
    "priors_t = bilby.core.prior.dict.ConditionalPriorDict()\n",
    "# define the smooth function related priors \n",
    "priors_t['n_exp'] = tbilby.DiscreteUniform(1,n_exp,'n_exp')\n",
    "priors_t  = tbilby.create_plain_priors(LogUniform,'A',n_exp,prior_dict_to_add=priors_t,minimum=1e-30, maximum=1e-13)  \n",
    "priors_t = tbilby.create_transdimensional_priors(TransdimensionalConditionalUniform_lamda,'lamda',nmax=n_exp,nested_conditional_transdimensional_params=['lamda'],conditional_params=[],prior_dict_to_add=priors_t,minimum=-10,maximum=2)\n",
    "# define the line functions related priors \n",
    "priors_t['n_lorentzian'] = tbilby.DiscreteUniform(0,n_peaks,'n_lorentzian')\n",
    "priors_t = tbilby.create_plain_priors(LogUniform,'gam',n_peaks,prior_dict_to_add=priors_t,minimum=0.01, maximum=1)  \n",
    "priors_t = tbilby.create_transdimensional_priors(TransdimensionalConditionalTransInterped_loc,'loc',nmax=n_peaks,nested_conditional_transdimensional_params=['loc'],conditional_params=['n_lorentzian'],prior_dict_to_add=priors_t,xx=x,yy=kernel_new(x),SaveConditionFunctionsToFile=False)\n",
    "priors_t_temp = tbilby.create_transdimensional_priors(TransdimensionalConditionalBeta_Amp,'Amp',nmax=n_peaks,nested_conditional_transdimensional_params=['Amp'],conditional_transdimensional_params=['loc',{'A':n_exp,'lamda':n_exp}],conditional_params=['n_exp'],SaveConditionFunctionsToFile=True,alpha = 4, beta=0.2,minimum=10,maximum=1000)\n",
    "# here we use another flexability of tbilby, sending in some data into the prior.    \n",
    "for k in priors_t_temp.keys():\n",
    "    priors_t_temp[k].set_extra_data(x,y)    \n",
    "    priors_t[k] = priors_t_temp[k] # set the priors accordingly \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y= get_GW_data(f_i=450,f_f=550)\n",
    "\n",
    "likelihood = ASD_GaussianLikelihood(x, y, model)\n",
    "\n",
    "\n",
    "result = bilby.run_sampler(\n",
    "        likelihood=likelihood,\n",
    "        priors=priors_t,\n",
    "        sample='rslice',\n",
    "        nlive=500,\n",
    "        walks=50,\n",
    "        outdir='outdir',\n",
    "        label=label,\n",
    "        clean=True,\n",
    "        resume=False,\n",
    "        npool=16,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344da5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results \n",
    "\n",
    "result = bilby.result.read_in_result(filename=label+'_result.json')\n",
    "result_processed,cols = tbilby.core.base.preprocess_results(result,componant_functions_dict,remove_ghost_samples=True,return_samples_of_most_freq_component_function=True)\n",
    "best_params_post = tbilby.core.base.extract_maximal_likelihood_param_values(result_processed, model)\n",
    "\n",
    "needed_params = infer_parameters_from_function(model)\n",
    "\n",
    "model_parameters = {k: best_params_post[k] for k in needed_params}\n",
    "\n",
    "plt.loglog(x,y,'-o',label='Data')\n",
    "plt.loglog(x,model(x,**model_parameters),'-m',label='Max likelihhod curve')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
